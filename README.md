# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
The dataset that was given contained bank marketing data and the task was a using logistic regression model to determine if the client would subscribe to a term deposit.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
In this projec we used the HyperDrive to tune the hyperparameters of the Logistic Regression model.  We also used AutoML to find the most most optimized model for the same dataset.  The best performing model that AutoML came up with was "VotingEnsemble" with accuracy of 0.9188


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
There were the steps taken:
1) Import data using TabularDatasetFactory
2) Cleaning of data. One-hot encoding of categorical features and preprocessing of date
3) Splitting of data into train and test data
4) Using scikit-learn logistic regression model for classification
5) Create the RunScriptConfig and passed the necessary paramenters such as the training script, the enviornment and the compute target for the our job
6) Configure Hyperdrive:
    a) Parameter Sampler
    b) Primary Metric Selection
    c) Early termination policy
    d) Resources and Configuration Run script which includes the estimator
7) Save the trained model

As specified above, we have used logistic regression model to perfom binary classification and hyperdrive tool to choose the best hyperparameter values from the parameter search space. Under the hood logistic regression uses a sigmoidal function to estimate the probabilities between the dependent/target variable and one or more independent variables(features).

**What are the benefits of the parameter sampler you chose?**

The chosen parameter sampler was RandomParameterSampling because it supports both discrete and continuous hyperparameters. In random sampling , the hyperparameter (C : smaller values specify stronger regularization, max_iter : maximum number of iterations taken for the solvers to converge) values are randomly selected from the defined search space. The biggest benefit of RandomSampling is it choose hyperparamters randmoly thus reducing the computational time and complexity.


**What are the benefits of the early stopping policy you chose?**

The early stopping policy I chose was BanditPolicy because it is based on slack factor and evaluation interval. the benefit of Bandit is that it terminates the runs where the primary metric is not within the specified slack factor compared to the best performing run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
